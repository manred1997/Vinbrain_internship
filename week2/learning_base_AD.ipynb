{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from colorama import Fore\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('bert-base-uncased\\\\tokenizer_config.json',\n",
       " 'bert-base-uncased\\\\special_tokens_map.json',\n",
       " 'bert-base-uncased\\\\vocab.txt',\n",
       " 'bert-base-uncased\\\\added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "slow_tokenizer.save_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "# tokenizer = BertWordPieceTokenizer(\"week2/tokenzier/vocab.txt\", lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# slow_tokenizer.save_pretrained(\"bert_base_uncased/\")\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased/vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AAAI-21-SDU-shared-task-2-AD-master/dataset/train.json\") as f:\n",
    "    train = json.load(f)\n",
    "with open(\"AAAI-21-SDU-shared-task-2-AD-master/dataset/diction.json\") as f:\n",
    "    diction = json.load(f)\n",
    "with open(\"AAAI-21-SDU-shared-task-2-AD-master/dataset/dev.json\") as f:\n",
    "    dev = json.load(f)\n",
    "with open(\"AAAI-21-SDU-shared-task-2-AD-master/dataset/predictions.json\") as f:\n",
    "    predictions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50034\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6189\n"
     ]
    }
   ],
   "source": [
    "print(len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6189\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(list_token):\n",
    "    return list(map(lambda x: x.lower(), list_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in train:\n",
    "    s[\"tokens\"] = normalize(s[\"tokens\"])\n",
    "    s[\"text\"] = \" \".join(s[\"tokens\"])\n",
    "    start_char_idx = 0\n",
    "    for i in range(0, s[\"acronym\"]):\n",
    "        start_char_idx += len(s[\"tokens\"][i])\n",
    "        start_char_idx += 1\n",
    "    s[\"start_char_idx\"] = start_char_idx\n",
    "    s[\"len_acronym\"] = len(s[\"tokens\"][s[\"acronym\"]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in dev:\n",
    "    s[\"tokens\"] = normalize(s[\"tokens\"])\n",
    "    s[\"text\"] = \" \".join(s[\"tokens\"])\n",
    "    start_char_idx = 0\n",
    "    for i in range(0, s[\"acronym\"]):\n",
    "        start_char_idx += len(s[\"tokens\"][i])\n",
    "        start_char_idx += 1\n",
    "    s[\"start_char_idx\"] = start_char_idx\n",
    "    s[\"len_acronym\"] = len(s[\"tokens\"][s[\"acronym\"]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, tokenizer, expansion, context, start_char_idx, len_acronym, max_seq_lenght=384):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.expansion = expansion\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.len_acronym = len_acronym\n",
    "        self.max_seq_lenght = max_seq_lenght\n",
    "        self.skip = False\n",
    "        \n",
    "        self.start_token_idx = -1\n",
    "        self.end_token_idx = -1\n",
    "        \n",
    "    def preprocess(self):\n",
    "        tokenized_expansion = self.tokenizer.encode(self.expansion)\n",
    "        tokenized_context = self.tokenizer.encode(self.context)\n",
    "        \n",
    "        end_char_idx = self.start_char_idx + self.len_acronym\n",
    "        if end_char_idx >= len(self.context): \n",
    "            self.skip = True\n",
    "            return\n",
    "        \n",
    "        is_char_in_context = [0]*len(self.context)\n",
    "        for idx in range(self.start_char_idx, end_char_idx):\n",
    "            is_char_in_context[idx] = 1\n",
    "        \n",
    "        arc_token_idx  = []\n",
    "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "            if sum(is_char_in_context[start:end]) > 0: arc_token_idx.append(idx)\n",
    "        if len(arc_token_idx) == 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        self.start_token_idx = arc_token_idx[0]\n",
    "        self.end_token_idx = arc_token_idx[-1]\n",
    "        \n",
    "        input_ids = tokenized_context.ids + tokenized_expansion.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_expansion.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        \n",
    "        padding_length = self.max_seq_lenght - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([0]* padding_length)\n",
    "            token_type_ids = token_type_ids + ([0]* padding_length)\n",
    "            attention_mask = attention_mask + ([0]* padding_length)\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        \n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples(raw_data, desc, tokenizer):\n",
    "    p_bar = tqdm(total=len(raw_data), desc=desc,\n",
    "                 position=0, leave=True,\n",
    "                 file=sys.stdout, bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.BLUE, Fore.RESET))\n",
    "    examples = []\n",
    "    for item in raw_data:\n",
    "        expansion = item[\"expansion\"]\n",
    "        context = item[\"text\"]\n",
    "        start_char_idx = item[\"start_char_idx\"]\n",
    "        len_acronym = item[\"len_acronym\"]\n",
    "        example = Sample(tokenizer, expansion, context, start_char_idx, len_acronym)\n",
    "        example.preprocess()\n",
    "        examples.append(example)\n",
    "        p_bar.update(1)\n",
    "    p_bar.close()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating training points: 100%|\u001b[34m██████████\u001b[39m| 50034/50034 [00:24<00:00, 2022.38it/s]\n"
     ]
    }
   ],
   "source": [
    "examples = create_examples(train, \"Creating training points\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_targets(examples):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in examples:\n",
    "        if item.skip is False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "        \n",
    "    x = [dataset_dict[\"input_ids\"], dataset_dict[\"token_type_ids\"], dataset_dict[\"attention_mask\"]]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = create_inputs_targets(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(49880, 384)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "x_train[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}